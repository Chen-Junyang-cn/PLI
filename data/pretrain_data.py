import json
from pathlib import Path
from typing import List, Optional, Union, Dict, Literal
import PIL
import PIL.Image
from torchvision import transforms
from torch.utils.data import Dataset
import pandas as pd


class ImageNetDataset(Dataset):
    """
    Dataset class for ImageNet dataset
    The dataset yields a dictionary with the following keys: ['image', 'image_name']
    """

    def __init__(self, dataset_path: Union[Path, str] = "data/ImageNet1K",
                 split='test', img_transform=None, text_transform=None):
        dataset_path = Path(dataset_path)
        self.dataset_path = dataset_path
        self.preprocess = img_transform
        self.text_transform = text_transform

        if split not in ['train', 'val', 'test']:
            raise ValueError("split should be in ['train', 'val', 'test']")

        self.images_path = [p for p in (dataset_path / split).rglob("*.JPEG")]
        print("-"*20, "ImageNet dataset length:", len(self.images_path), "-"*20)

    def __getitem__(self, index) -> dict:
        img_path = self.images_path[index]
        img = PIL.Image.open(img_path).convert('RGB')
        if self.preprocess:
            img = self.preprocess(img)

        txt_path = self.dataset_path / 'captions' / (img_path.name.replace('JPEG', 'txt'))
        with open(txt_path, 'r') as f:
            txt = f.read()
        if self.text_transform:
            txt = self.text_transform(txt)
        return {
            'image': img,
            'image_name': img_path.name,
            'caption': txt
        }

    def __len__(self):
        return len(self.images_path)

    @classmethod
    def code(cls):
        return "ImageNet"



class FashionIQCaptionsDataset(Dataset):
    """
    Dataset class for FashionIQ captions dataset, caption is generated by BLIP2
    """
    def __init__(self, dataset_path: Union[Path, str] = "data/fashionIQcaptions", clothing_type='dress',
                 split='train', img_transform=None, text_transform=None):
        self.dataset_path = Path(dataset_path)
        self.image_path = dataset_path.replace('captions', '/image_data')
        self.clothing_type = clothing_type

        self.preprocess = img_transform
        self.text_transform = text_transform
        # only use train split
        with open(self.dataset_path / f'cap.{clothing_type}.{split}.json', 'r') as f:
            self.data = json.load(f) # [{"candidate": "id", "captions": "text"}, ...]

    def __getitem__(self, index) -> dict:
        img_path = Path(self.image_path) / self.clothing_type / (self.data[index]['candidate'] + '.jpg')
        img = PIL.Image.open(img_path).convert('RGB')
        if self.preprocess:
            img = self.preprocess(img)

        txt = self.data[index]['captions']
        if self.text_transform:
            txt = self.text_transform(txt)

        return {
            'image': img,
            'image_name': img_path.name,
            'caption': txt
        }

    def __len__(self):
        return len(self.data)

    @classmethod
    def code(cls):
        return "FashionIQcaptions"

    @classmethod
    def all_subset_codes(cls):
        return ['dress', 'shirt', 'toptee']


class MSCOCOCaptionsDataset(Dataset):
    """
    Dataset class for MSCOCO captions dataset
    """
    def __init__(self, dataset_path: Union[Path, str] = "data/MSCOCO", split='train',
                 img_transform=None, text_transform=None):
        self.dataset_path = Path(dataset_path)
        self.split = split

        self.preprocess = img_transform
        self.text_transform = text_transform
        # only use train split
        with open(self.dataset_path / f'annotations/captions_{split}2017.json', 'r') as f:
            data = json.load(f) # ['info', 'licenses', 'images', 'annotations']
        # multiple captions for a image, 591753 captions
        self.annotations = data['annotations'] # list(dict_keys(['image_id', 'id', 'caption']))

        # print number of images, 118287 images
        image_ids = [ann['image_id'] for ann in self.annotations]
        print("-"*20, "MSCOCO dataset #image:", len(list(set(image_ids))), "-"*20)


    def __getitem__(self, index) -> dict:
        img_id = self.annotations[index]['image_id']
        img_path = self.dataset_path / f'{self.split}2017' / f'{img_id:012d}.jpg'
        img = PIL.Image.open(img_path).convert('RGB')
        if self.preprocess:
            img = self.preprocess(img)

        txt = self.annotations[index]['caption']
        if self.text_transform:
            txt = self.text_transform(txt)

        return {
            'image': img,
            'image_name': img_path.name,
            'caption': txt
        }

    def __len__(self):
        return len(self.annotations)

    @classmethod
    def code(cls):
        return "MSCOCO"

    @classmethod
    def all_subset_codes(cls):
        return ['train', 'val']


class ImageNetMscocoDataset(Dataset):
    """
    Dataset class for MSCOCO + ImageNet
    """
    def __init__(self, dataset_path: Union[Path, str] = "data", split='train',
                 img_transform=None, text_transform=None):
        self.mscoco = Flickr30kCaptionsDataset(split='train', img_transform=img_transform, text_transform=text_transform)
        self.imagenet = ImageNetDataset(img_transform=img_transform, text_transform=text_transform)

    def __getitem__(self, index) -> dict:
        if index < len(self.mscoco):
            return self.mscoco[index]
        else:
            return self.imagenet[index - len(self.mscoco)]
        
    def __len__(self):
        return len(self.mscoco) + len(self.imagenet)

    @classmethod
    def code(cls):
        return "ImageCOCO"
    
class Flickr30kCaptionsDataset(Dataset):
    """
    Dataset class for Flickr30k captions dataset
    """
    def __init__(self, dataset_path: Union[Path, str] = "data/flickr30k", split='train',
                 img_transform=None, text_transform=None):
        self.dataset_path = Path(dataset_path)
        self.split = split

        self.preprocess = img_transform
        self.text_transform = text_transform
        # only use train split, images in flickr30k_images, captions in results.csv
        # data = pd.read_csv(self.dataset_path / 'results.csv', sep="|") # image_name	comment_number	comment
        # new_columns = {
        #     " comment_number": "comment_number",
        #     " comment": "comment"
        # }
        # self.data = data.rename(columns=new_columns)
        # columns = ['raw', 'sentids', 'split', 'filename', 'img_id'], "raw" is the caption list
        data = pd.read_csv(self.dataset_path / 'flickr_annotations_30k.csv')
        self.img_path, self.txt = [], []
        for idx, row in data.iterrows():
            for i in eval(row['raw']):
                self.img_path.append(row['filename'])
                self.txt.append(i)


        print("-"*20, "Flickr30k dataset length:", len(self.img_path), "-"*20)
    def __getitem__(self, index) -> dict:
        # img_path = self.dataset_path / 'flickr30k_images' / self.data.iloc[index]['image_name']
        img_path = self.dataset_path / 'flickr30k_images' / self.img_path[index]
        img = PIL.Image.open(img_path).convert('RGB')
        if self.preprocess:
            img = self.preprocess(img)

        # txt = str(self.data.iloc[index]['comment'])
        txt = self.txt[index]
        if isinstance(txt, float):
            print(txt)
        if self.text_transform:
            txt = self.text_transform(txt)

        return {
            'image': img,
            'image_name': img_path.name,
            'caption': txt
        }

    def __len__(self):
        return len(self.img_path)

    @classmethod
    def code(cls):
        return "Flickr30k"

    @classmethod
    def all_subset_codes(cls):
        return ['train']

class LlavaDataset(Dataset):
    """
    Dataset class for Llava dataset
    """
    def __init__(self, dataset_path: Union[Path, str] = "data/llava", split='train',
                 img_transform=None, text_transform=None):
        self.dataset_path = Path(dataset_path)
        with open(self.dataset_path / 'metadata.json', 'r') as f:
            self.metadata = json.load(f)
        self.preprocess = img_transform
        self.text_transform = text_transform

        print("-"*20, "Llava dataset length:", len(self.metadata), "-"*20)

    def __getitem__(self, index) -> dict:
        img_path = self.dataset_path / "images" / self.metadata[index]['image']
        img = PIL.Image.open(img_path).convert('RGB')
        if self.preprocess:
            img = self.preprocess(img)

        txt = self.metadata[index]['caption']
        if self.text_transform:
            txt = self.text_transform(txt)

        return {
            'image': img,
            'image_name': img_path.name,
            'caption': txt
        }
    def __len__(self):
        return len(self.metadata)

    @classmethod
    def code(cls):
        return "llava"

class ImageVaDataset(Dataset):
    """
    Dataset class for ImageNet and Llava Dataset
    """
    def __init__(self, dataset_path: Union[Path, str] = "data", split='train',
                 img_transform=None, text_transform=None):
        self.imagenet = ImageNetDataset(img_transform=img_transform, text_transform=text_transform)
        self.llava = LlavaDataset(img_transform=img_transform, text_transform=text_transform)

    def __getitem__(self, index) -> dict:
        if index < len(self.imagenet):
            return self.imagenet[index]
        else:
            return self.llava[index - len(self.imagenet)]
    
    def __len__(self):
        return len(self.imagenet) + len(self.llava)

    @classmethod
    def code(cls):
        return "imageva"

# test
if __name__ == "__main__":

    preprocess = transforms.Compose([
        transforms.Resize(224),
        transforms.CenterCrop(224),
        transforms.ToTensor(),
    ])
    dataset = ImageNetDataset("./ImageNet1K", img_transform=preprocess)
    print(len(dataset))
    print(dataset[5]["image"].shape)
    print(len([p for p in (Path("./ImageNet1K/captions")).rglob("*.txt")]))

    dataset = FashionIQCaptionsDataset("./fashionIQcaptions", clothing_type='dress', split='train', img_transform=preprocess)
    print(dataset[0], len(dataset))

    # read MSCOCO captions
    dataset = MSCOCOCaptionsDataset("./MSCOCO", split='train', img_transform=preprocess)
    print(dataset[0], len(dataset))

    # anno = pd.read_csv("/data/chenjy/code/TGIR/Diffusion4TGIR/data/flickr_annotations_30k.csv")
    # print(anno)
    # read Flickr30k captions
    dataset = Flickr30kCaptionsDataset("./flickr30k", split='train', img_transform=preprocess)
    # print(dataset.data.head())
    print(dataset[0])
    dataset = LlavaDataset("./llava", img_transform=preprocess)
    print(len(dataset))
    print(dataset[0])